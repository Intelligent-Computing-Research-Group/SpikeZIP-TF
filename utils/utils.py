# -*- coding: utf-8 -*-
"""
Created on Sun Oct 25 00:24:07 2020

@author: Jiang Yuxin
"""

import torch
import torch.nn as nn
from torch.nn import CrossEntropyLoss
import time
from tqdm import tqdm
from sklearn.metrics import (
    roc_auc_score,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    classification_report
)
from copy import deepcopy

def Metric(y_true, y_pred):
    """
    compute and show the classification result
    """
    accuracy = accuracy_score(y_true, y_pred)
    macro_precision = precision_score(y_true, y_pred, average='macro')
    macro_recall = recall_score(y_true, y_pred, average='macro')
    weighted_f1 = f1_score(y_true, y_pred, average='macro')
    target_names = ['class_0', 'class_1']
    report = classification_report(y_true, y_pred, target_names=target_names, digits=3)

    print('Accuracy: {:.1%}\nPrecision: {:.1%}\nRecall: {:.1%}\nF1: {:.1%}'.format(accuracy, macro_precision,
                                                                                   macro_recall, weighted_f1))
    print("classification_report:\n")
    print(report)


def correct_predictions(output_probabilities, targets):
    """
    Compute the number of predictions that match some target classes in the
    output of a model.
    Args:
        output_probabilities: A tensor of probabilities for different output
            classes.
        targets: The indices of the actual target classes.
    Returns:
        The number of correct predictions in 'output_probabilities'.
    """
    _, out_classes = output_probabilities.max(dim=1)
    correct = (out_classes == targets).sum()
    return correct.item()


def train(model, dataloader, optimizer, epoch_number, max_gradient_norm):
    """
    Train a model for one epoch on some input data with a given optimizer and
    criterion.
    Args:
        model: A torch module that must be trained on some input data.
        dataloader: A DataLoader object to iterate over the training data.
        optimizer: A torch optimizer to use for training on the input model.
        epoch_number: The number of the epoch for which training is performed.
        max_gradient_norm: Max. norm for gradient norm clipping.
    Returns:
        epoch_time: The total time necessary to train the epoch.
        epoch_loss: The training loss computed for the epoch.
        epoch_accuracy: The accuracy computed for the epoch.
    """
    # Switch the model to train mode.
    model.train()
    device = model.device
    epoch_start = time.time()
    batch_time_avg = 0.0
    running_loss = 0.0
    correct_preds = 0
    tqdm_batch_iterator = tqdm(dataloader)
    for batch_index, (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in enumerate(tqdm_batch_iterator):
        batch_start = time.time()
        # Move input and output data to the GPU if it is used.
        seqs, masks, segments, labels = batch_seqs.to(device), batch_seq_masks.to(device), batch_seq_segments.to(
            device), batch_labels.to(device)
        optimizer.zero_grad()
        loss, logits, probabilities = model(seqs, masks, segments, labels)
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), max_gradient_norm)
        optimizer.step()
        batch_time_avg += time.time() - batch_start
        running_loss += loss.item()
        correct_preds += correct_predictions(probabilities, labels)
        description = "Avg. batch proc. time: {:.4f}s, loss: {:.4f}" \
            .format(batch_time_avg / (batch_index + 1), running_loss / (batch_index + 1))
        tqdm_batch_iterator.set_description(description)
    epoch_time = time.time() - epoch_start
    epoch_loss = running_loss / len(dataloader)
    epoch_accuracy = correct_preds / len(dataloader.dataset)
    return epoch_time, epoch_loss, epoch_accuracy


# def validate(model, dataloader):
#     """
#     Compute the loss and accuracy of a model on some validation dataset.
#     Args:
#         model: A torch module for which the loss and accuracy must be
#             computed.
#         dataloader: A DataLoader object to iterate over the validation data.
#     Returns:
#         epoch_time: The total time to compute the loss and accuracy on the
#             entire validation set.
#         epoch_loss: The loss computed on the entire validation set.
#         epoch_accuracy: The accuracy computed on the entire validation set.
#         roc_auc_score(all_labels, all_prob): The auc computed on the entire validation set.
#         all_prob: The probability of classification as label 1 on the entire validation set.
#     """
#     # Switch to evaluate mode.
#     model.eval()
#     device = model.device
#     epoch_start = time.time()
#     running_loss = 0.0
#     running_accuracy = 0.0
#     all_prob = []
#     all_labels = []
#     # Deactivate autograd for evaluation.
#     with torch.no_grad():
#         for (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in dataloader:
#             # Move input and output data to the GPU if one is used.
#             seqs = batch_seqs.to(device)
#             masks = batch_seq_masks.to(device)
#             segments = batch_seq_segments.to(device)
#             labels = batch_labels.to(device)
#             loss, logits, probabilities = model(seqs, masks, segments, labels)
#             running_loss += loss.item()
#             running_accuracy += correct_predictions(probabilities, labels)
#             all_prob.extend(probabilities[:, 1].cpu().numpy())
#             all_labels.extend(batch_labels)
#     epoch_time = time.time() - epoch_start
#     epoch_loss = running_loss / len(dataloader)
#     epoch_accuracy = running_accuracy / (len(dataloader.dataset))
#     return epoch_time, epoch_loss, epoch_accuracy, roc_auc_score(all_labels, all_prob), all_prob


def validate(model, dataloader,args=None):
    """
    Compute the loss and accuracy of a model on some validation dataset.
    Args:
        model: A torch module for which the loss and accuracy must be
            computed.
        dataloader: A DataLoader object to iterate over the validation data.
    Returns:
        epoch_time: The total time to compute the loss and accuracy on the
            entire validation set.
        epoch_loss: The loss computed on the entire validation set.
        epoch_accuracy: The accuracy computed on the entire validation set.
        roc_auc_score(all_labels, all_prob): The auc computed on the entire validation set.
        all_prob: The probability of classification as label 1 on the entire validation set.
    """
    # Switch to evaluate mode.
    model.eval()
    device = model.device
    epoch_start = time.time()
    running_loss = 0.0
    running_accuracy = 0.0
    all_prob = []
    all_labels = []
    criterion = CrossEntropyLoss()
    correct_per_timestep_all = []
    total_num = 0
    max_T = 0
    # Deactivate autograd for evaluation.
    with torch.no_grad():
        for (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in tqdm(dataloader):
            # Move input and output data to the GPU if one is used.
            seqs = batch_seqs.to(device)
            total_num = total_num + seqs.shape[0]
            masks = batch_seq_masks.to(device)
            segments = batch_seq_segments.to(device)
            labels = batch_labels.to(device)
            if args.mode == "SNN":
                output, count, accu_per_timestep = model(seqs, masks, segments, labels, verbose=True)
                # print(accu_per_timestep.shape, count)
                max_T = max(max_T, count)
                print(max_T)
                if accu_per_timestep.shape[0] < max_T:
                    padding_per_timestep = accu_per_timestep[-1].unsqueeze(0)
                    padding_length = max_T - accu_per_timestep.shape[0]
                    accu_per_timestep = torch.cat(
                        [accu_per_timestep, padding_per_timestep.repeat(padding_length, 1, 1)], dim=0)

                _, predicted_per_time_step = torch.max(accu_per_timestep.data, 2)
                correct_per_timestep = torch.sum((predicted_per_time_step == labels.unsqueeze(0)), dim=1)
                
                # statistic the total correct sample per time-step
                for i in range(len(correct_per_timestep)):
                    if i >= len(correct_per_timestep_all):
                        if i == 0:
                            correct_per_timestep_all.append(0)
                        else:
                            correct_per_timestep_all.append(correct_per_timestep_all[-1])

                for i in range(len(correct_per_timestep)):
                    correct_per_timestep_all[i] = correct_per_timestep_all[i] + correct_per_timestep[i]
                    
                loss = criterion(output, labels)
                probabilities = nn.functional.softmax(output)
                running_loss += loss.item()
                running_accuracy += correct_predictions(probabilities, labels)
                all_prob.extend(probabilities[:, 1].cpu().numpy())
                all_labels.extend(batch_labels)
                acc_per_timestep_all = []
                for i in range(len(correct_per_timestep_all)):
                    acc_per_timestep_all.append(correct_per_timestep_all[i]/total_num * 100)
                print("mid: acc_per_timestep",acc_per_timestep_all)
                print("mid: running_accuracy",running_accuracy/total_num * 100)
                model.reset()
            else:
                loss, logits, probabilities = model(seqs, masks, segments, labels)
                running_loss += loss.item()
                running_accuracy += correct_predictions(probabilities, labels)
                all_prob.extend(probabilities[:, 1].cpu().numpy())
                all_labels.extend(batch_labels)
    epoch_time = time.time() - epoch_start
    epoch_loss = running_loss / len(dataloader)
    epoch_accuracy = running_accuracy / (len(dataloader.dataset))
    f = open(f"{args.target_dir}/accuracy_per_timestep.txt","w+")
    for i in range(len(correct_per_timestep_all)):
        f.write(f"T={i}, acc={correct_per_timestep_all[i]/len(dataloader.dataset) * 100}%\n")
    f.close()
    return epoch_time, epoch_loss, epoch_accuracy, all_prob


def test(model, dataloader):
    """
    Test the accuracy of a model on some labelled test dataset.
    Args:
        model: The torch module on which testing must be performed.
        dataloader: A DataLoader object to iterate over some dataset.
    Returns:
        batch_time: The average time to predict the classes of a batch.
        total_time: The total time to process the whole dataset.
        accuracy: The accuracy of the model on the input data.
        all_prob: The probability of classification as label 1 on the entire validation set.
    """
    # Switch the model to eval mode.
    model.eval()
    device = model.device
    time_start = time.time()
    batch_time = 0.0
    accuracy = 0.0
    all_prob = []
    all_labels = []
    # Deactivate autograd for evaluation.
    with torch.no_grad():
        for (batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels) in dataloader:
            batch_start = time.time()
            # Move input and output data to the GPU if one is used.
            seqs, masks, segments, labels = batch_seqs.to(device), batch_seq_masks.to(device), batch_seq_segments.to(
                device), batch_labels.to(device)
            _, _, probabilities = model(seqs, masks, segments, labels)
            accuracy += correct_predictions(probabilities, labels)
            batch_time += time.time() - batch_start
            all_prob.extend(probabilities[:, 1].cpu().numpy())
            all_labels.extend(batch_labels)
    batch_time /= len(dataloader)
    total_time = time.time() - time_start
    accuracy /= (len(dataloader.dataset))

    return batch_time, total_time, accuracy, all_prob

